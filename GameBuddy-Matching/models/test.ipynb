{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/burak/Developer/MachineLearning/GameBuddy/GameBuddy-Matching/dataset/gamers-info.csv')\n",
    "# drop result column\n",
    "df = df.drop('result', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games = df['games']\n",
    "df_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode games by using one hot encoding\n",
    "df_games_encoded = df_games.str.get_dummies(sep=',')\n",
    "# remove if there is a [ or ] in the game name\n",
    "df_games_encoded = df_games_encoded.rename(columns=lambda x: x.replace('[', '').replace(']', ''))\n",
    "df_games_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_encoded = df['keywords'].str.get_dummies(sep=',')\n",
    "df_keywords_encoded = df_keywords_encoded.rename(columns=lambda x: x.replace('[', '').replace(']', ''))\n",
    "df_keywords_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine df_games_encoded and df_keywords_encoded to df\n",
    "df = df.drop(['games', 'keywords'], axis=1)\n",
    "print(df.shape)\n",
    "df = df.join(df_games_encoded)\n",
    "print(df.shape)\n",
    "df = df.join(df_keywords_encoded)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['country'] = le.fit_transform(df['country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# scale all the columns except\n",
    "# country and result\n",
    "df_scaled = df.copy()\n",
    "df_scaled = df_scaled.drop(['country', 'age'], axis=1)\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_scaled), columns=df_scaled.columns)\n",
    "\n",
    "df_scaled.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "pca_df = pca.fit_transform(df_scaled)\n",
    "# fit PCA on the scaled data\n",
    "pca.fit(df_scaled)\n",
    "\n",
    "# find the accuracy of pca\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pca.fit_transform(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_explained_variance = pca.explained_variance_ratio_.cumsum()\n",
    "n_over_95 = len(total_explained_variance[total_explained_variance >= 0.65])\n",
    "n_to_reach_95 = df_scaled.shape[1] - n_over_95\n",
    "\n",
    "print(total_explained_variance.size)\n",
    "print(n_over_95)\n",
    "print(n_to_reach_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number features: {n_to_reach_95}\\nTotal Variance Explained: {total_explained_variance[n_over_95]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_over_95)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 20\n",
    "cluster_cnt = [i for i in range(2, n_clusters, 2)]\n",
    "\n",
    "scores = []\n",
    "db_scores = []\n",
    "\n",
    "for n in cluster_cnt:\n",
    "    hac = AgglomerativeClustering(n_clusters=n)\n",
    "    hac.fit(df_pca)\n",
    "\n",
    "    score = silhouette_score(df_pca, hac.labels_)\n",
    "    db_scores.append(davies_bouldin_score(df_pca, hac.labels_))\n",
    "    scores.append(silhouette_score(df_pca, hac.labels_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of cluster_cnt: \", len(cluster_cnt))\n",
    "print(\"Size of scores: \", len(scores))\n",
    "print(\"Size of db_scores: \", len(db_scores))\n",
    "print(\"Hac lables: \", hac.labels_.size)\n",
    "print(\"Size of df\", df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(columns=['Cluster Score'], index=[i for i in range(2, len(db_scores)+2)])\n",
    "#add a new column at the begging of df named cluster score and assign the hac labels to it\n",
    "print(\"Before merge: \", df.shape)\n",
    "df.insert(0, 'Cluster Score', hac.labels_)\n",
    "print(\"After merge: \", df.shape)\n",
    "\n",
    "print('Max Value:\\nCluster #', df[df['Cluster Score'] == df['Cluster Score'].max()])\n",
    "print('\\nMin Value:\\nCluster #', df[df['Cluster Score'] == df['Cluster Score'].min()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(open('matching.pkl', 'wb')) as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "with(open('games-encoded.pkl', 'wb')) as f:\n",
    "    pickle.dump(df_games_encoded, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
